{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to ETL standard Mimic-III dataset to FHIR format\n",
    "Sequentially construct each new fhir resource table\n",
    "\n",
    "### MIMIC-III tables to FHIR resource mapping\n",
    "||Original format | FHIR resource| Progress|Final Check|\n",
    "|------|:-----|:-----|:-----|---:---|\n",
    "|1|patients | patient| C,L,A|Done|\n",
    "|2a|admissions | encounter| C,L,A|Done|\n",
    "|2b|diagnoses_icd | encounter| C,L,A|Done|\n",
    "|3|icustay | enounter|C,L,A|Done|\n",
    "|4|cptevents | claim| C,L,A|Done|\n",
    "|5|noteevents | diagnosticReport| C,L,A|Done|\n",
    "|6|inputevents_cv | medicationDispense| C,L,A|Done|\n",
    "|7|inputevents_mv | medicationDispense| C,L,A|Done|\n",
    "|8|prescriptions | medicationRequest| C,L,A|Done|\n",
    "|9|chartevents | observation| C,L,A|Done|\n",
    "|10|datetimeevents | observation| C,L,A|Done|\n",
    "|11|labevents | observation| C,L,A|Done|\n",
    "|12|caregivers | practitioner| C,L,A|Done|\n",
    "|13|procedures_icd | procedure| C,L,A|Done|\n",
    "|14|procedureevents_mv | procedure| C,L,A|Done|\n",
    "|15|microbiology | specimen| C,L,A|Done|\n",
    "|16|outputevents | specimen| C,L,A|Done|\n",
    "|17|service | serviceRequest|C,L,A|Done|\n",
    "|18|callout | -|-|-|\n",
    "|19|transfers | -|-|-|\n",
    "|20|drgcodes | -|-|-|\n",
    "\n",
    "- Could convert all tables to resource bundles.\n",
    "- Any resource attributes that are dictionaries/objects themselves are flattened out using the x_y notation to represent entry y of attribute x.\n",
    "- Any resource attributes with multiple names are concatenated in the following manner: 'Service Provider' -> 'serviceProvider'\n",
    "\n",
    "- Python the INT datatype cannot be a NaN value and columns are therefore casted to floats. One solution to keep numbers as ints is to replace NaN with -1:\n",
    "- - medicationRequest.partOf.replace(np.nan,-1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getcwd().split('/')[1] == 'data':\n",
    "    # Using docker environment\n",
    "    data_path = '/data/data/mimic/orig_sample/'\n",
    "    output_path = '/data/data/mimic/fhir_sample/'\n",
    "    leonhard = False\n",
    "elif 'cluster' in os.getcwd().split('/'):\n",
    "    # Using Euler or Leonhard cluster\n",
    "    data_path = '/cluster/home/lfranz/master_thesis/data/physionet.org/files/mimiciii/1.4/'\n",
    "    output_path = '/cluster/scratch/lfranz/data/mimic/fhir/'\n",
    "    leonhard = True\n",
    "\n",
    "data_files = os.listdir(data_path)\n",
    "#display(data_files)\n",
    "\n",
    "compression = 'gzip' if data_files[0][-3:] == '.gz' else None\n",
    "file_ext = '.csv.gz' if compression else '.csv'\n",
    "print('File compression: {}'.format(compression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create each FHIR resource type table individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.patients table\n",
    "\n",
    "#### MAPPING:<br>\n",
    "\n",
    "- As soon as this table is joineed with another table the identifier column needs to be renamed to 'subject'\n",
    "- We could add the deathtime from the admissions table, which could be more accurate.\n",
    "- Check https://www.hl7.org/fhir/valueset-languages.html for language codes(probably easiest to just create a bigger version defining the language the language name is in and the language name itself)\n",
    "- Using MARITIAL_STATUS from last encounter\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.patients.SUBJECT_ID | fhir.patient.identifier|\n",
    "|2|mimic.patients.GENDER | fhir.patient.gender|\n",
    "|3|mimic.patients.DOB | fhir.patient.birthDate|\n",
    "|4|mimic.patients.DOD | fhir.patient.deceasedDateTime|\n",
    "|5|mimic.admissions.LANGUAGE | fhir.patient.communication_language|\n",
    "|6|mimic.admissions.MARITIAL_STATUS | fhir.patient.maritialStatus|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_patients(data_path, output_path):\n",
    "    patients = pd.read_csv(data_path+'PATIENTS'+file_ext, compression=compression, usecols=['SUBJECT_ID', 'GENDER', 'DOB', 'DOD'])\n",
    "    admissions = pd.read_csv(data_path+'ADMISSIONS'+file_ext, compression=compression, usecols=['SUBJECT_ID','LANGUAGE','MARITAL_STATUS'])\n",
    "    \n",
    "    # Add the marital status and language information from the admissions dataframe by using the information from the last admission.\n",
    "    ad_temp = admissions.groupby('SUBJECT_ID').tail(1)\n",
    "    patients = pd.merge(patients, ad_temp, on='SUBJECT_ID')\n",
    "\n",
    "    # Rename columns to FHIR names\n",
    "    patients.rename(columns={'SUBJECT_ID':'identifier',\n",
    "                             'GENDER':'gender', \n",
    "                             'DOB':'birthDate', \n",
    "                             'DOD':'deceasedDateTime',\n",
    "                             'LANGUAGE':'communication_language',\n",
    "                             'MARITAL_STATUS':'maritalStatus'}, inplace=True)\n",
    "\n",
    "    # Output to csv\n",
    "    patients.to_csv(output_path+'patient.csv.gz', compression='gzip', index=False)\n",
    "    return patients\n",
    "\n",
    "patient_fhir = transform_patients(data_path, output_path)\n",
    "patient_fhir.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.encounter table\n",
    "\n",
    "#### ADMISSIONS MAPPING:<br>\n",
    "\n",
    "- location column (add all interim locations and periods)\n",
    "- - Could add location field by using Transfer and SERVICES mimictables\n",
    "- - Combine both mimic.admissions.EDREGTIME and mimic.admissions.EDOUTTIME to add time period for fhir.encounter.locations list\n",
    "- mimic.admissions.INSURANCE - Might be better placed in claims fhir table\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.admissions.SUBJECT_ID|fhir.encounter.subject|\n",
    "|2|mimic.admissions.HADM_ID|fhir.encounter.identifier|\n",
    "|3|'admission'|fhir.encounter.serviceType|\n",
    "|4|mimic.admissions.ADMITTIME|fhir.encounter.period_start|\n",
    "|5|mimic.admissions.DISCHTIME|fhir.encounter.period_end|\n",
    "|6|mimic.admissions.(DISCHTIME - ADMITTIME)|fhir.encounter.length|\n",
    "|7|mimic.admissions.ADMISSION_TYPE|fhir.encounter.type|\n",
    "|8|mimic.admissions.ADMISSION_LOCATION|fhir.encounter.hospitalization_origin|\n",
    "|9|mimic.admissions.DISCHARGE_LOCATION|fhir.encounter.hospitalization_destination|\n",
    "|10|mimic.admissions.INSURANCE|fhir.encounter.serviceProvider|\n",
    "|11|mimic.DIAGNOSES_ICD.ICD9_CODE|fhir.encounter.diagnosis_condition|\n",
    "|12|mimic.DIAGNOSES_ICD.SEQ_NUM|fhir.encounter.diagnosis_condition|\n",
    "|13|mimic.admissions.DIAGNOSIS|fhir.encounter.diagnosis_description|\n",
    "\n",
    "#### ICUSTAYS MAPPING:<br>\n",
    "\n",
    "- Could improve location field by using Transfer table\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.icustays.SUBJECT_ID|fhir.encounter.subject|\n",
    "|2|mimic.icustays.HADM_ID|fhir.encounter.partOf|\n",
    "|3|mimic.icustays.ICUSTAY_ID|fhir.encounter.identifier|\n",
    "|4|'icustay'|fhir.encounter.serviceType|\n",
    "|5|mimic.icustays.INTIME|fhir.encounter.period_start|\n",
    "|6|mimic.icustays.OUTTIME|fhir.encounter.period_end|\n",
    "|7|mimic.icustays.(OUTTIME - INTIME)|fhir.encounter.length|\n",
    "|8|mimic.icustays.DBSOURCE|fhir.encounter.type|\n",
    "|9|mimic.icustays.FIRST_CAREUNIT|fhir.encounter.location|\n",
    "|10|mimic.icustays.LAST_CAREUNIT|fhir.encounter.location|\n",
    "|11|mimic.icustays.FIRST_WARDID|fhir.encounter.location_id|\n",
    "|12|mimic.icustays.LAST_WARDID|fhir.encounter.location_id|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_admissions(data_path, output_path):\n",
    "    admissions = pd.read_csv(data_path+'ADMISSIONS'+file_ext, compression=compression, usecols=['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'DISCHARGE_LOCATION', 'INSURANCE', 'DIAGNOSIS'])\n",
    "    diagnoses_icd = pd.read_csv(data_path+'DIAGNOSES_ICD'+file_ext, compression=compression)\n",
    "    \n",
    "    # Convert times to datetime formats and add length column\n",
    "    admissions.ADMITTIME = pd.to_datetime(admissions.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    admissions.DISCHTIME = pd.to_datetime(admissions.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    admissions['length'] = admissions['DISCHTIME']-admissions['ADMITTIME']\n",
    "\n",
    "    # Add diagnoses icd list. Icd codes automatically in the order of priority (descending priority), which is important for billing.\n",
    "    diagnoses_icd_list = diagnoses_icd.groupby('HADM_ID')['ICD9_CODE'].apply(list).reset_index(name='diagnosis_condition')\n",
    "    admissions = pd.merge(admissions, diagnoses_icd_list, on='HADM_ID')\n",
    "    \n",
    "    # Define encounter service type\n",
    "    admissions['serviceType'] = 'admission'\n",
    "    \n",
    "    # Rename columns to FHIR names\n",
    "    admissions.rename(columns={'SUBJECT_ID':'subject',\n",
    "                               'HADM_ID':'identifier', \n",
    "                               'ADMITTIME':'period_start', \n",
    "                               'DISCHTIME':'period_end', \n",
    "                               'ADMISSION_TYPE':'type',\n",
    "                               'ADMISSION_LOCATION':'hospitalization_origin',\n",
    "                               'DISCHARGE_LOCATION':'hospitalization_destination',\n",
    "                               'INSURANCE':'serviceProvider',\n",
    "                               'DIAGNOSIS':'diagnosis_description'}, inplace=True)\n",
    "    \n",
    "    admissions = admissions.reindex(columns=['subject', \n",
    "                                             'identifier', \n",
    "                                             'period_start', \n",
    "                                             'period_end',\n",
    "                                             'length',\n",
    "                                             'type',\n",
    "                                             'hospitalization_origin',\n",
    "                                             'hospitalization_destination',\n",
    "                                             'serviceProvider',\n",
    "                                             'diagnosis_condition',\n",
    "                                             'diagnosis_description'], copy=False)\n",
    "    \n",
    "    # Output to csv\n",
    "    admissions.to_csv(output_path+'encounter.csv.gz', compression='gzip', index=False)\n",
    "    return admissions\n",
    "\n",
    "encounter_admissions = transform_admissions(data_path, output_path)\n",
    "encounter_admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_icustays(data_path, output_path):\n",
    "    icustays = pd.read_csv(data_path+'ICUSTAYS'+file_ext, compression=compression)    \n",
    "\n",
    "    # Convert times to datetime formats and add length column\n",
    "    icustays.INTIME = pd.to_datetime(icustays.INTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    icustays.OUTTIME = pd.to_datetime(icustays.OUTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    icustays['length'] = icustays['OUTTIME']-icustays['INTIME']\n",
    "\n",
    "    # Combine ADMISSION_LOCATION and DISCHARGE_LOCATION to location list\n",
    "    icustays['location'] = icustays[['FIRST_CAREUNIT', 'LAST_CAREUNIT']].values.tolist()\n",
    "    icustays.drop(['FIRST_CAREUNIT', 'LAST_CAREUNIT'], axis=1, inplace=True)\n",
    "    icustays['location_id'] = icustays[['FIRST_WARDID', 'LAST_WARDID']].values.tolist()\n",
    "    icustays.drop(['FIRST_WARDID', 'LAST_WARDID'], axis=1, inplace=True)\n",
    "    \n",
    "    # Define encounter service type\n",
    "    icustays['serviceType'] = 'icustay'\n",
    "\n",
    "    # Rename columns to FHIR names\n",
    "    icustays.rename(columns={'SUBJECT_ID':'subject',\n",
    "                             'HADM_ID':'partOf', \n",
    "                             'ICUSTAY_ID':'identifier',\n",
    "                             'DBSOURCE':'type',\n",
    "                             'INTIME':'period_start', \n",
    "                             'OUTTIME':'period_end'}, inplace=True)\n",
    "    \n",
    "    icustays = icustays.reindex(columns=['subject', \n",
    "                                         'partOf',\n",
    "                                         'identifier', \n",
    "                                         'period_start', \n",
    "                                         'period_end',\n",
    "                                         'length',\n",
    "                                         'type',\n",
    "                                         'serviceType',\n",
    "                                         'location',\n",
    "                                         'location_id'], copy=False)\n",
    "    \n",
    "    # Output to csv\n",
    "    icustays.to_csv(output_path+'encounter_icustays.csv.gz', compression='gzip', index=False)\n",
    "    return icustays\n",
    "\n",
    "encounter_icustays = transform_icustays(data_path, output_path)\n",
    "encounter_icustays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.claim table\n",
    "\n",
    "#### MAPPING: <br>\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.cptevents.ROW_ID|fhir.claim.identifier|\n",
    "|2|mimic.cptevents.SUBJECT_ID|fhir.claim.subject|\n",
    "|3|mimic.cptevents.HADM_ID|fhir.claim.encounter|\n",
    "|4|mimic.cptevents.COSTCENTER|fhir.claim.provider|\n",
    "|5|mimic.cptevents.CHARTDATE|fhir.claim.created|\n",
    "|6|mimic.cptevents.CPT_NUMBER|fhir.claim.item_category_cpt_num|\n",
    "|7|mimic.cptevents.CPT_SUFFIX|fhir.claim.item_category_cpt_str|\n",
    "|8|mimic.cptevents.TICKET_ID_SEQ|fhir.claim.item_sequence|\n",
    "|9|mimic.cptevents.DESCRIPTION|fhir.claim.item_detail|\n",
    "|10|mimic.cptevents.SECTIONHEADER|fhir.claim.type|\n",
    "|11|mimic.cptevents.SUBSECTIONHEADER|fhir.claim.subType|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_cptevents(data_path, output_path):\n",
    "    cptevents = pd.read_csv(data_path+'CPTEVENTS'+file_ext, compression=compression)\n",
    "    cptevents.CHARTDATE = pd.to_datetime(cptevents.CHARTDATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "\n",
    "    # Drop extra columns\n",
    "    cptevents.drop(['CPT_CD'], axis=1, inplace=True)\n",
    "\n",
    "    cptevents.rename(columns={'ROW_ID':'identifier',\n",
    "                              'SUBJECT_ID':'subject', \n",
    "                              'HADM_ID':'encounter', \n",
    "                              'CHARTDATE':'created', \n",
    "                              'SECTIONHEADER':'type',\n",
    "                              'SUBSECTIONHEADER':'subType',\n",
    "                              'COSTCENTER':'provider',\n",
    "                              'CPT_NUMBER':'item_category_cpt_num',\n",
    "                              'CPT_SUFFIX':'item_category_cpt_str',\n",
    "                              'TICKET_ID_SEQ':'item_sequence',\n",
    "                              'DESCRIPTION':'item_detail'}, inplace=True)\n",
    "\n",
    "    cptevents.to_csv(output_path+'claim.csv.gz', compression='gzip', index=False)\n",
    "    return cptevents\n",
    "\n",
    "claims = transform_cptevents(data_path, output_path)\n",
    "claims.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.diagnosticReport table\n",
    "\n",
    "#### MAPPING: <br>\n",
    "\n",
    "- DESCRIPTION isn't a code but comes closest to the idea of a concept.\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.notevents.ROW_ID|fhir.diagnosticReport.identifier|\n",
    "|2|mimic.notevents.SUBJECT_ID|fhir.diagnosticReport.subject|\n",
    "|3|mimic.notevents.HADM_ID|fhir.diagnosticReport.encounter|\n",
    "|4|mimic.notevents.CHARTDATE|fhir.diagnosticReport.effectiveDateTime|\n",
    "|5|mimic.notevents.CGID|fhir.diagnosticReport.performer|\n",
    "|6|mimic.notevents.CATEGORY|fhir.diagnosticReport.category|\n",
    "|7|mimic.notevents.DESCRIPTION|fhir.diagnosticReport.codeDisplay|\n",
    "|8|mimic.notevents.TEXT|fhir.diagnosticReport.presentedForm|\n",
    "|9|mimic.notevents.ISERROR| fhir.diagnosticReport.status_error|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_noteevents(data_path, output_path):\n",
    "    noteevents = pd.read_csv(data_path+'NOTEEVENTS'+file_ext, compression=compression)\n",
    "    \n",
    "    noteevents.CHARTDATE = pd.to_datetime(noteevents.CHARTDATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "\n",
    "    # Drop extra columns\n",
    "    noteevents.drop(['CHARTTIME', 'STORETIME'], axis=1, inplace=True)\n",
    "    \n",
    "    noteevents.rename(columns={'ROW_ID':'identifier',\n",
    "                                'SUBJECT_ID':'subject',\n",
    "                                'HADM_ID':'encounter',\n",
    "                                'CHARTDATE':'effectiveDateTime',\n",
    "                                'CGID':'performer',\n",
    "                                'CATEGORY':'category',\n",
    "                                'DESCRIPTION':'codeDisplay',\n",
    "                                'TEXT':'presentedForm',\n",
    "                                'ISERROR':'status_error'}, inplace=True)\n",
    "\n",
    "    noteevents.to_csv(output_path+'diagnosticReport.csv.gz', compression='gzip', index=False) \n",
    "    return noteevents\n",
    "\n",
    "diagnosticReport = transform_noteevents(data_path, output_path)\n",
    "diagnosticReport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.medicationDispense table\n",
    "\n",
    "#### INPUTEVENTS_CV MAPPING:\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.inputevents_cv.ROW_ID|fhir.medicationDispense.identifier|\n",
    "|2|mimic.inputevents_cv.SUBJECT_ID | fhir.medicationDispense.subject|\n",
    "|3|mimic.inputevents_cv.HADM_ID | fhir.medicationDispense.encounter|\n",
    "|4|mimic.inputevents_cv.ICUSTAY_ID | fhir.medicationDispense.partOf|\n",
    "|5|mimic.inputevents_cv.ITEMID | fhir.medicationDispense.medicationCodeableConcept|\n",
    "|6|mimic.inputevents_cv.CHARTTIME | fhir.medicationDispense.whenHandedOver|\n",
    "|7|mimic.inputevents_cv.AMOUNT | fhir.medicationDispense.valueQuantity|\n",
    "|8|mimic.inputevents_cv.AMOUNTUOM | fhir.medicationDispense.unit|\n",
    "|9|mimic.inputevents_cv.RATE | fhir.medicationDispense.dosageRate|\n",
    "|10|mimic.inputevents_cv.RATEUOM | fhir.medicationDispense.dosageRate_unit|\n",
    "|11|mimic.inputevents_cv.CGID | fhir.medicationDispense.performer|\n",
    "|12|mimic.inputevents_cv.ORDERID | fhir.medicationDispense.type|\n",
    "|13|mimic.inputevents_cv.LINKORDERID | fhir.medicationDispense.type_sub|\n",
    "|14|mimic.inputevents_cv.STOPPED | fhir.medicationDispense.status|\n",
    "|15|mimic.inputevents_cv.ORIGINALAMOUNT | fhir.medicationDispense.dosageOriginal_amount|\n",
    "|16|mimic.inputevents_cv.ORIGINALAMOUNTUOM|fhir.medicationDispense.dosageOriginal_amountUnit|\n",
    "|17|mimic.inputevents_cv.ORIGNALROUTE | fhir.medicationDispense.dosageOriginal_route|\n",
    "|18|mimic.inputevents_cv.ORIGINALRATE | fhir.medicationDispense.dosageOriginal_rate|\n",
    "|19|mimic.inputevents_cv.ORIGINALRATEUOM | fhir.medicationDispense.dosageOriginal_rateUnit|\n",
    "|20|mimic.inputevents_cv.ORIGINALSITE | fhir.medicationDispense.dosageOriginal_site|\n",
    "|21|mimic.inputevents_cv.NEWBOTTLE | fhir.medicationDispense.note|\n",
    "|22|mimic.d_items.(LABEL+DBSOURCE+PARAM_TYPE) | fhir.medicationDispense.note|\n",
    "|23|mimic.d_items.CATEGORY | fhir.medicationDispense.category|\n",
    "\n",
    "#### INPUTEVENTS_MV MAPPING:\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.inputevents_mv.ROW_ID|fhir.medicationDispense.identifier|\n",
    "|2|mimic.inputevents_mv.SUBJECT_ID|fhir.medicationDispense.subject|\n",
    "|3|mimic.inputevents_mv.HADM_ID|fhir.medicationDispense.encounter|\n",
    "|4|mimic.inputevents_mv.ICUSTAY_ID|fhir.medicationDispense.partOf|\n",
    "|5|mimic.inputevents_mv.ITEMID|fhir.medicationDispense.medicationCodeableConcept|\n",
    "|6|mimic.inputevents_mv.STARTTIME|fhir.medicationDispense.whenHandedOver_start|\n",
    "|7|mimic.inputevents_mv.ENDTIME|fhir.medicationDispense.whenHandedOver_end|\n",
    "|8|mimic.inputevents_mv.AMOUNT|fhir.medicationDispense.valueQuantity|\n",
    "|9|mimic.inputevents_mv.AMOUNTUOM|fhir.medicationDispense.unit|\n",
    "|10|mimic.inputevents_mv.RATE|fhir.medicationDispense.dosageRate|\n",
    "|11|mimic.inputevents_mv.RATEUOM|fhir.medicationDispense.dosageRate_unit|\n",
    "|12|mimic.inputevents_mv.CGID|fhir.medicationDispense.performer|\n",
    "|13|mimic.inputevents_mv.ORDERID|fhir.medicationDispense.type|\n",
    "|14|mimic.inputevents_mv.LINKORDERID|fhir.medicationDispense.type_sub|\n",
    "|15|mimic.inputevents_mv.ORDERCATEGORYNAME|fhir.medicationDispense.supportingInformation_order_catName|\n",
    "|16|mimic.inputevents_mv.SECONDARYORDERCATEGORYNAME|fhir.medicationDispense.supportingInformation_order_secCatName|\n",
    "|17|mimic.inputevents_mv.ORDERCOMPONENTTYPEDESCRIPTION|fhir.medicationDispense.supportingInformation_order_desc_componentTyped|\n",
    "|18|mimic.inputevents_mv.ORDERCATEGORYDESCRIPTION|fhir.medicationDispense.supportingInformation_order_desc_cat|\n",
    "|19|mimic.inputevents_mv.PATIENTWEIGHT|fhir.medicationDispense.supportingInformation_patientWeight|\n",
    "|20|mimic.inputevents_mv.ORIGINALAMOUNT|fhir.medicationDispense.dosageInstruction_original_amount|\n",
    "|21|mimic.inputevents_mv.ORIGINALRATE|fhir.medicationDispense.dosageInstruction_original_rate|\n",
    "|22|mimic.inputevents_mv.TOTALAMOUNT|fhir.medicationDispense.dosageInstruction_total_amount|\n",
    "|23|mimic.inputevents_mv.TOTALAMOUNTUOM|fhir.medicationDispense.dosageInstruction_total_unit|\n",
    "|24|mimic.inputevents_mv.ISOPENBAG|fhir.medicationDispense.dosageInstruction_openBag|\n",
    "|25|mimic.inputevents_mv.CONTINUEINNEXTDEPT|fhir.medicationDispense.eventHistory_contExtDep|\n",
    "|26|mimic.inputevents_mv.CANCELREASON|fhir.medicationDispense.detectedIssue_code|\n",
    "|27|mimic.inputevents_mv.STATUSDESCRIPTION|fhir.medicationDispense.status|\n",
    "|28|mimic.inputevents_mv.COMMENTS_EDITEDBY|fhir.medicationDispense.performer_comment_edit|\n",
    "|29|mimic.inputevents_mv.COMMENTS_CANCELEDBY|fhir.medicationDispense.performer_comment_cancel|\n",
    "|30|mimic.inputevents_mv.COMMENTS_DATE|fhir.medicationDispense.detectedIssue_date|\n",
    "|31|mimic.d_items.(LABEL+DBSOURCE+PARAM_TYPE)|fhir.medicationDispense.note|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputevents_cv(data_path, output_path):\n",
    "    inputevents_cv = pd.read_csv(data_path+'INPUTEVENTS_CV'+file_ext, compression=compression)\n",
    "    d_items = pd.read_csv(data_path+'D_ITEMS'+file_ext, compression=compression, index_col=0)\n",
    "    \n",
    "    medicationDispense = pd.merge(inputevents_cv, d_items, on='ITEMID')\n",
    "    medicationDispense.CHARTTIME = pd.to_datetime(medicationDispense.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "    medicationDispense['NEWBOTTLE'].replace(np.NaN, 0, inplace=True)\n",
    "    medicationDispense['PARAM_TYPE'].replace(np.NaN, '', regex=True, inplace=True)\n",
    "    medicationDispense['note'] = medicationDispense['LABEL'] + ' ' + medicationDispense['DBSOURCE'] + ' ' + medicationDispense['PARAM_TYPE'] + ' ' + medicationDispense['NEWBOTTLE'].astype(str) +' new bottle'\n",
    "\n",
    "\n",
    "    medicationDispense.rename(columns={'ROW_ID':'identifier',\n",
    "                                       'SUBJECT_ID':'subject',\n",
    "                                       'HADM_ID':'encounter',\n",
    "                                       'ICUSTAY_ID':'partOf',\n",
    "                                       'CHARTTIME':'whenHandedOver',\n",
    "                                       'ITEMID':'medicationCodeableConcept',\n",
    "                                       'AMOUNT':'valueQuantity',\n",
    "                                       'AMOUNTUOM':'unit',\n",
    "                                       'RATE':'dosageRate',\n",
    "                                       'RATEUOM':'dosageRate_unit',\n",
    "                                       'CGID':'performer',\n",
    "                                       'ORDERID':'type',\n",
    "                                       'LINKORDERID':'type_sub',\n",
    "                                       'STOPPED':'status',\n",
    "                                       'ORIGINALAMOUNT':'dosageOriginal_amount',\n",
    "                                       'ORIGINALAMOUNTUOM':'dosageOriginal_amountUnit',\n",
    "                                       'ORIGINALROUTE':'dosageOriginal_route',\n",
    "                                       'ORIGINALRATE':'dosageOriginal_rate',\n",
    "                                       'ORIGINALRATEUOM':'dosageOriginal_rateUnit',\n",
    "                                       'ORIGINALSITE':'dosageOriginal_site',\n",
    "                                       'CATEGORY':'category'}, inplace=True)\n",
    "\n",
    "    medicationDispense.drop(['LABEL', 'PARAM_TYPE', 'STORETIME', 'ABBREVIATION', 'DBSOURCE', 'LINKSTO', 'CONCEPTID', 'UNITNAME', 'NEWBOTTLE'], axis=1, inplace=True)\n",
    "\n",
    "    medicationDispense.to_csv(output_path + 'medicationDispense.csv.gz', compression='gzip', index=False)\n",
    "\n",
    "    return medicationDispense\n",
    "\n",
    "medicationDispense_cv = transform_inputevents_cv(data_path, output_path)\n",
    "medicationDispense_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputevents_mv(data_path, output_path):\n",
    "    inputevents_mv = pd.read_csv(data_path+'INPUTEVENTS_MV'+file_ext, compression=compression)\n",
    "    d_items = pd.read_csv(data_path+'D_ITEMS'+file_ext, compression=compression, index_col=0)\n",
    "    \n",
    "    medicationDispense = pd.merge(inputevents_mv, d_items, on='ITEMID')\n",
    "    \n",
    "    medicationDispense.STARTTIME = pd.to_datetime(medicationDispense.STARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    medicationDispense.ENDTIME = pd.to_datetime(medicationDispense.ENDTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    medicationDispense.COMMENTS_DATE = pd.to_datetime(medicationDispense.COMMENTS_DATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "\n",
    "    medicationDispense['PARAM_TYPE'].replace(np.NaN, '', regex=True, inplace=True)\n",
    "    medicationDispense['note'] = medicationDispense['LABEL'] + ' ' + medicationDispense['DBSOURCE'] + ' ' + medicationDispense['PARAM_TYPE']\n",
    "    \n",
    "    medicationDispense.drop(['STORETIME','LABEL', 'PARAM_TYPE', 'ABBREVIATION', 'DBSOURCE', 'LINKSTO', 'CONCEPTID', 'UNITNAME'], axis=1, inplace=True)\n",
    "\n",
    "    medicationDispense.rename(columns={'ROW_ID':'identifier',\n",
    "                                       'SUBJECT_ID':'subject',\n",
    "                                       'HADM_ID':'encounter',\n",
    "                                       'ICUSTAY_ID':'partOf',\n",
    "                                       'STARTTIME':'whenHandedOver_start',\n",
    "                                       'ENDTIME':'whenHandedOver_end',\n",
    "                                       'ITEMID':'medicationCodeableConcept',\n",
    "                                       'AMOUNT':'valueQuantity',\n",
    "                                       'AMOUNTUOM':'unit',\n",
    "                                       'RATE':'dosageRate',\n",
    "                                       'RATEUOM':'dosageRate_unit',\n",
    "                                       'CGID':'performer',\n",
    "                                       'ORDERID':'type',\n",
    "                                       'LINKORDERID':'type_sub',\n",
    "                                       'ORDERCATEGORYNAME':'supportingInformation_order_catName',\n",
    "                                       'SECONDARYORDERCATEGORYNAME':'supportingInformation_order_secCatName', \n",
    "                                       'ORDERCOMPONENTTYPEDESCRIPTION':'supportingInformation_order_desc_componentTyped',\n",
    "                                       'ORDERCATEGORYDESCRIPTION':'supportingInformation_order_desc_cat', \n",
    "                                       'PATIENTWEIGHT':'supportingInformation_patientWeight', \n",
    "                                       'TOTALAMOUNT':'dosageInstruction_total_amount',\n",
    "                                       'TOTALAMOUNTUOM':'dosageInstruction_total_unit', \n",
    "                                       'ISOPENBAG':'dosageInstruction_openBag', \n",
    "                                       'CONTINUEINNEXTDEPT':'eventHistory_contExtDep', \n",
    "                                       'CANCELREASON':'detectedIssue_code',\n",
    "                                       'STATUSDESCRIPTION':'status', \n",
    "                                       'COMMENTS_EDITEDBY':'performer_comment_edit', \n",
    "                                       'COMMENTS_CANCELEDBY':'performer_comment_cancel',\n",
    "                                       'COMMENTS_DATE':'detectedIssue_date',\n",
    "                                       'ORIGINALAMOUNT':'dosageInstruction_original_amount',\n",
    "                                       'ORIGINALRATE':'dosageInstruction_original_rate',\n",
    "                                       'CATEGORY':'category'}, inplace=True)\n",
    "    \n",
    "    medicationDispense.to_csv(output_path + 'medicationDispense_mv.csv.gz', compression='gzip', index=False)\n",
    "    return medicationDispense\n",
    "\n",
    "medicationDispense_mv = transform_inputevents_mv(data_path, output_path)\n",
    "medicationDispense_mv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.medicationRequest table\n",
    "\n",
    "#### MAPPING: <br>\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.prescriptions.ROW_ID | fhir.medicationRequest.identifier|\n",
    "|2|mimic.prescriptions.SUBJECT_ID | fhir.medicationRequest.subject|\n",
    "|3|mimic.prescriptions.HADM_ID| fhir.medicationRequest.encounter|\n",
    "|4|mimic.prescriptions.ICUSTAY_ID | fhir.medicationRequest.partOf|\n",
    "|5|mimic.prescriptions.STARTDATE | fhir.medicationRequest.dispenseRequest_start|\n",
    "|6|mimic.prescriptions.ENDDATE | fhir.medicationRequest.dispenseRequest_end|\n",
    "|7|mimic.prescriptions.DRUG_TYPE | fhir.medicationRequest.category|\n",
    "|8|mimic.prescriptions.DRUG | fhir.medicationRequest.medication_name|\n",
    "|9|mimic.prescriptions.DRUG_NAME_GENERIC | fhir.medicationRequest.medication_genericName|\n",
    "|10|mimic.prescriptions.FORMULARY_DRUG_CD | fhir.medicationRequest.medication_code_CD|\n",
    "|11|mimic.prescriptions.GSN | fhir.medicationRequest.medication_code_GSN|\n",
    "|12|mimic.prescriptions.NDC | fhir.medicationRequest.medication_code_NDC|\n",
    "|13|mimic.prescriptions.DOSE_VAL_RX | fhir.medicationRequest.dosageInstruction_value|\n",
    "|14|mimic.prescriptions.DOSE_UNIT_RX | fhir.medicationRequest.dosageInstruction_unit|\n",
    "|15|mimic.prescriptions.FORM_VAL_DISP | fhir.medicationRequest.dispenseRequest_value|\n",
    "|16|mimic.prescriptions.FORM_UNIT_DISP | fhir.medicationRequest.dispenseRequest_unit|\n",
    "|17|mimic.prescriptions.ROUTE | fhir.medicationRequest.courseOfTherapyType|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_prescriptions(data_path, output_path):\n",
    "    prescriptions = pd.read_csv(data_path+'PRESCRIPTIONS'+file_ext, compression=compression)\n",
    "\n",
    "    prescriptions.STARTDATE = pd.to_datetime(prescriptions.STARTDATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "    prescriptions.ENDDATE = pd.to_datetime(prescriptions.ENDDATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "    \n",
    "    # Drop extra columns\n",
    "    prescriptions.drop(['DRUG_NAME_POE', 'PROD_STRENGTH'], axis=1, inplace=True)\n",
    "\n",
    "    prescriptions.rename(columns={'ROW_ID':'identifier',\n",
    "                                  'SUBJECT_ID':'subject', \n",
    "                                  'HADM_ID':'encounter', \n",
    "                                  'ICUSTAY_ID':'partOf',\n",
    "                                  'STARTDATE':'dispenseRequest_start', \n",
    "                                  'ENDDATE':'dispenseRequest_end',\n",
    "                                  'DRUG_TYPE':'category', \n",
    "                                  'DRUG':'medication_name', \n",
    "                                  'DRUG_NAME_GENERIC':'medication_genericName',\n",
    "                                  'FORMULARY_DRUG_CD':'medication_code_CD', \n",
    "                                  'GSN':'medication_code_GSN', \n",
    "                                  'NDC':'medication_code_NDC', \n",
    "                                  'DOSE_VAL_RX':'dosageInstruction_value',\n",
    "                                  'DOSE_UNIT_RX':'dosageInstruction_unit', \n",
    "                                  'FORM_VAL_DISP':'dispenseRequest_value', \n",
    "                                  'FORM_UNIT_DISP':'dispenseRequest_unit', \n",
    "                                  'ROUTE':'courseOfTherapyType'}, inplace=True)\n",
    "\n",
    "    prescriptions.to_csv(output_path+'medicationRequest.csv.gz', compression='gzip', index=False)\n",
    "    return prescriptions\n",
    "\n",
    "medicationRequest = transform_prescriptions(data_path, output_path)\n",
    "medicationRequest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.observation table\n",
    "\n",
    "#### CHARTEVENTS MAPPING:<br>\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "||mimic.chartevents.ROW_ID| fhir.observation.identifier|\n",
    "||mimic.chartevents.SUBJECT_ID | fhir.observation.subject|\n",
    "||mimic.chartevents.HADM_ID | fhir.observation.encounter|\n",
    "||mimic.chartevents.ICUSTAY_ID | fhir.observation.partOf|\n",
    "||mimic.chartevents.ITEMID | fhir.observation.code|\n",
    "||mimic.chartevents.CHARTTIME | fhir.observation.effectiveDateTime|\n",
    "||mimic.chartevents.CGID | fhir.observation.performer|\n",
    "||mimic.chartevents.VALUE | fhir.observation.value|\n",
    "||mimic.chartevents.VALUENUM | fhir.observation.value_quantity|\n",
    "||mimic.chartevents.VALUEUOM | fhir.observation.unit|\n",
    "||mimic.chartevents.WARNING | fhir.observation.interpretation|\n",
    "||mimic.chartevents.RESULTSTATUS | fhir.observation.status|\n",
    "||mimic.d_items.(LABEL+DBSOURCE+PARAM_TYPE) | fhir.observation.note|\n",
    "||mimic.d_items.CATEGORY | fhir.observation.category_sub|\n",
    "||'chartevents' | fhir.observation.category|\n",
    "\n",
    "\n",
    "#### DATETIMEEVENTS MAPPING:<br>\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "||mimic.datetimeevents.ROW_ID| fhir.observation.identifier|\n",
    "||mimic.datetimeevents.SUBJECT_ID | fhir.observation.subject|\n",
    "||mimic.datetimeevents.HADM_ID | fhir.observation.encounter|\n",
    "||mimic.datetimeevents.ICUSTAY_ID | fhir.observation.partOf|\n",
    "||mimic.datetimeevents.ITEMID | fhir.observation.code|\n",
    "||mimic.datetimeevents.CHARTTIME | fhir.observation.effectiveDateTime|\n",
    "||mimic.datetimeevents.CGID | fhir.observation.performer|\n",
    "||mimic.datetimeevents.VALUE | fhir.observation.value|\n",
    "||mimic.datetimeevents.VALUEUOM | fhir.observation.unit|\n",
    "||mimic.datetimeevents.WARNING | fhir.observation.interpretation|\n",
    "||mimic.datetimeevents.RESULTSTATUS | fhir.observation.status|\n",
    "||mimic.d_items.(LABEL+DBSOURCE+PARAM_TYPE) | fhir.observation.note|\n",
    "||mimic.d_items.CATEGORY | fhir.observation.category_sub|\n",
    "||'datetimeevents' | fhir.observation.category|\n",
    "\n",
    "\n",
    "#### LABEVENTS MAPPING:<br>\n",
    "\n",
    "- Consider assigning loinc_code to code not to method. LOINC_CODE would first need to be assigned, which isn't straightforward.\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "||mimic.labevents.ROW_ID|fhir.observation.identifier|\n",
    "||mimic.labevents.SUBJECT_ID|fhir.observation.subject|\n",
    "||mimic.labevents.HADM_ID|fhir.observation.encounter|\n",
    "||mimic.labevents.CHARTTIME|fhir.observation.effectiveDateTime|\n",
    "||mimic.labevents.ITEMID | fhir.observation.code|\n",
    "||mimic.d_labitems.LOINC_CODE | fhir.observation.code_loinc|\n",
    "||mimic.labevents.VALUE | fhir.observation.value|\n",
    "||mimic.labevents.VALUENUM | fhir.observation.value_quantity|\n",
    "||mimic.labevents.VALUEUOM | fhir.observation.unit|\n",
    "||mimic.labevents.FLAG | fhir.observation.interpretation|\n",
    "||mimic.d_labitems.(LABEL+FLUID) | fhir.observation.note|\n",
    "||mimic.d_labitems.CATEGORY | fhir.observation.category_sub|\n",
    "||'labevents' | fhir.observation.category|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_chartevents(data_path, output_path):\n",
    "    chartevents = pd.read_csv(data_path+'CHARTEVENTS'+file_ext, compression=compression)\n",
    "    d_items = pd.read_csv(data_path+'D_ITEMS'+file_ext, compression=compression, index_col=0)\n",
    "    \n",
    "    observation_ce = pd.merge(chartevents, d_items, on='ITEMID')\n",
    "    \n",
    "    observation_ce.CHARTTIME = pd.to_datetime(observation_ce.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "    observation_ce['PARAM_TYPE'].replace(np.NaN, '', regex=True, inplace=True)\n",
    "    observation_ce['note'] = observation_ce['LABEL'] + ' ' + observation_ce['DBSOURCE'] + ' ' + observation_ce['PARAM_TYPE']\n",
    "\n",
    "    observation_ce.loc[observation_ce.STOPPED==\"D/C'd\",'RESULTSTATUS'] = 'discharged'\n",
    "    observation_ce.loc[observation_ce.ERROR==1,'RESULTSTATUS'] = 'Error'\n",
    "    # New columns to adapt to Chartevents observations\n",
    "    observation_ce['category']='chartevents'\n",
    "\n",
    "    observation_ce.drop(['LABEL', 'PARAM_TYPE', 'STORETIME', 'ERROR', 'ABBREVIATION', 'DBSOURCE', 'LINKSTO', 'CONCEPTID', 'STOPPED', 'UNITNAME'], axis=1, inplace=True)\n",
    "    \n",
    "    observation_ce.rename(columns={'ROW_ID':'identifier',\n",
    "                                   'SUBJECT_ID':'subject',\n",
    "                                   'HADM_ID':'encounter',                               \n",
    "                                   'ICUSTAY_ID':'partOf',\n",
    "                                   'ITEMID':'code',\n",
    "                                   'CGID':'performer',\n",
    "                                   'CHARTTIME':'effectiveDateTime',\n",
    "                                   'VALUE':'value',\n",
    "                                   'VALUENUM':'value_quantity',\n",
    "                                   'VALUEUOM':'unit',\n",
    "                                   'WARNING':'interpretation',\n",
    "                                   'RESULTSTATUS':'status',\n",
    "                                   'CATEGORY':'category_sub'}, inplace=True)\n",
    "    \n",
    "    observation_ce = observation_ce.reindex(columns=['identifier',\n",
    "                                                     'subject', \n",
    "                                                     'encounter', \n",
    "                                                     'partOf', \n",
    "                                                     'code',\n",
    "                                                     'effectiveDateTime',\n",
    "                                                     'performer',\n",
    "                                                     'value',\n",
    "                                                     'value_quantity',\n",
    "                                                     'unit', \n",
    "                                                     'interpretation',\n",
    "                                                     'status',\n",
    "                                                     'note',\n",
    "                                                     'category_sub',\n",
    "                                                     'category'], copy=False)\n",
    "\n",
    "    observation_ce.to_csv(output_path+'observation_ce.csv.gz', compression='gzip', index=False)\n",
    "    return observation_ce\n",
    "\n",
    "observation_ce = transform_chartevents(data_path, output_path)\n",
    "observation_ce.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_datetimeevents(data_path, output_path):\n",
    "    datetimeevents = pd.read_csv(data_path+'DATETIMEEVENTS'+file_ext, compression=compression)\n",
    "    d_items = pd.read_csv(data_path+'D_ITEMS'+file_ext, compression=compression, index_col=0)\n",
    "    \n",
    "    observation_dte = pd.merge(datetimeevents, d_items, on='ITEMID')\n",
    "    observation_dte.CHARTTIME = pd.to_datetime(observation_dte.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "    observation_dte['PARAM_TYPE'].replace(np.NaN, '', regex=True, inplace=True)\n",
    "    observation_dte['note'] = observation_dte['LABEL'] + ' ' + observation_dte['DBSOURCE'] + ' ' + observation_dte['PARAM_TYPE']\n",
    "\n",
    "    observation_dte.loc[observation_dte.STOPPED==\"D/C'd\",'RESULTSTATUS'] = 'Final'\n",
    "    observation_dte.loc[observation_dte.ERROR==1,'RESULTSTATUS'] = 'Error'\n",
    "    # New columns to adapt to DateTimeEvents observations\n",
    "    observation_dte['category']='datetimeevents'\n",
    "\n",
    "    observation_dte.drop(['LABEL', 'PARAM_TYPE', 'STORETIME', 'ERROR', 'ABBREVIATION', 'DBSOURCE', 'LINKSTO', 'CONCEPTID', 'STOPPED', 'UNITNAME'], axis=1, inplace=True)\n",
    "\n",
    "    observation_dte.rename(columns={'ROW_ID':'identifier',\n",
    "                                    'SUBJECT_ID':'subject',\n",
    "                                    'HADM_ID':'encounter',\n",
    "                                    'ICUSTAY_ID':'partOf',\n",
    "                                    'ITEMID':'code',\n",
    "                                    'CGID':'performer',\n",
    "                                    'CHARTTIME':'effectiveDateTime',\n",
    "                                    'VALUE':'value',\n",
    "                                    'VALUEUOM':'unit',\n",
    "                                    'WARNING':'interpretation',\n",
    "                                    'RESULTSTATUS':'status',\n",
    "                                    'CATEGORY':'category_sub'}, inplace=True)\n",
    "\n",
    "    observation_dte = observation_dte.reindex(columns=['identifier',\n",
    "                                                       'subject', \n",
    "                                                       'encounter', \n",
    "                                                       'partOf',\n",
    "                                                       'code',\n",
    "                                                       'effectiveDateTime', \n",
    "                                                       'performer',\n",
    "                                                       'value',\n",
    "                                                       'unit', \n",
    "                                                       'interpretation',\n",
    "                                                       'status',\n",
    "                                                       'note',\n",
    "                                                       'category_sub',\n",
    "                                                       'category'], copy=False)\n",
    "\n",
    "    observation_dte.to_csv(output_path+'observation_dte.csv.gz', compression='gzip', index=False)\n",
    "    return observation_dte\n",
    "\n",
    "observation_dte = transform_datetimeevents(data_path, output_path)\n",
    "observation_dte.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labevents(data_path, output_path):\n",
    "    labevents = pd.read_csv(data_path+'LABEVENTS'+file_ext, compression=compression)\n",
    "    d_labitems = pd.read_csv(data_path+'D_LABITEMS'+file_ext, compression=compression, index_col=0)\n",
    "\n",
    "    observation_le = pd.merge(labevents, d_labitems, on='ITEMID')\n",
    "    observation_le.CHARTTIME = pd.to_datetime(observation_le.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "    observation_le['note'] = observation_le['LABEL'] + ' ' + observation_le['FLUID']\n",
    "    observation_le.drop(['LABEL', 'FLUID'], axis=1, inplace=True)\n",
    "\n",
    "    # Add observation type\n",
    "    observation_le['category']='labevents'\n",
    "\n",
    "    observation_le.rename(columns={'ROW_ID':'identifier',\n",
    "                                   'SUBJECT_ID':'subject',\n",
    "                                   'HADM_ID':'encounter',\n",
    "                                   'ITEMID':'code',\n",
    "                                   'LOINC_CODE':'code_loinc',\n",
    "                                   'CHARTTIME':'effectiveDateTime',\n",
    "                                   'VALUE':'value',\n",
    "                                   'VALUENUM':'value_quantity',\n",
    "                                   'VALUEUOM':'unit',\n",
    "                                   'FLAG':'interpretation',\n",
    "                                   'CATEGORY':'category_sub'}, inplace=True)\n",
    "\n",
    "    observation_le = observation_le.reindex(columns=['identifier',\n",
    "                                                     'subject', \n",
    "                                                     'encounter',\n",
    "                                                     'effectiveDateTime',\n",
    "                                                     'code',\n",
    "                                                     'code_loinc',\n",
    "                                                     'value',\n",
    "                                                     'value_quantity',\n",
    "                                                     'unit', \n",
    "                                                     'interpretation',\n",
    "                                                     'note',\n",
    "                                                     'category_sub',\n",
    "                                                     'category'], copy=False)\n",
    "\n",
    "    observation_le.to_csv(output_path+'observation_le.csv.gz', compression='gzip', index=False)\n",
    "    return observation_le\n",
    "\n",
    "observation_le = transform_labevents(data_path, output_path)\n",
    "observation_le.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.practitioner table\n",
    "\n",
    "#### MAPPING:\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.caregivers.CGID|fhir.practitioner.identifier|\n",
    "|2|mimic.caregivers.LABEL|fhir.practitioner.qualification_category|\n",
    "|3|mimic.caregivers.DESCRIPTION|fhir.practitioner.qualification_label|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_caregivers(data_path, output_path):\n",
    "    caregivers = pd.read_csv(data_path+'CAREGIVERS'+file_ext, compression=compression)\n",
    "\n",
    "    caregivers.drop(['ROW_ID'], axis=1, inplace=True)\n",
    "\n",
    "    caregivers.rename(columns={'CGID':'identifier',\n",
    "                                 'LABEL':'qualification_label',\n",
    "                                 'DESCRIPTION':'qualification_category'}, inplace=True)\n",
    "\n",
    "    caregivers.to_csv(output_path+'practitioner.csv.gz', compression='gzip', index=False)\n",
    "    return caregivers\n",
    "\n",
    "practitioner = transform_caregivers(data_path, output_path)\n",
    "practitioner.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.procedure table\n",
    "\n",
    "#### PROCEDURES_ICD MAPPING:<br>\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.procedures_icd.ROW_ID | fhir.procedure.identifier|\n",
    "|2|mimic.procedures_icd.SUBJECT_ID | fhir.procedure.subject|\n",
    "|3|mimic.procedures_icd.HADM_ID | fhir.procedure.encounter|\n",
    "|4|mimic.procedures_icd.ICD9_CODE | fhir.procedure.code_icd9|\n",
    "\n",
    "#### PROCEDUREEVENTS_MV MAPPING:<br>\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.procedureevents_mv.ROW_ID | fhir.procedure.identifier|\n",
    "|2|mimic.procedureevents_mv.SUBJECT_ID | fhir.procedure.subject|\n",
    "|3|mimic.procedureevents_mv.HADM_ID | fhir.procedure.encounter|\n",
    "|4|mimic.procedureevents_mv.ICUSTAY_ID| fhir.procedure.partOf|\n",
    "|5|mimic.procedureevents_mv.STARTTIME| fhir.procedure.performedRange_start|\n",
    "|6|mimic.procedureevents_mv.ENDTIME| fhir.procedure.performedRange_end|\n",
    "|7|mimic.procedureevents_mv.ITEMID| fhir.procedure.code|\n",
    "|8|mimic.procedureevents_mv.VALUE| fhir.procedure.outcome_value|\n",
    "|9|mimic.procedureevents_mv.VALUEOM | fhir.procedure.outcome_unit|\n",
    "|10|mimic.procedureevents_mv.LOCATION| fhir.procedure.location_name|\n",
    "|11|mimic.procedureevents_mv.LOCATIONCATEGORY| fhir.procedure.location_category|\n",
    "|12|mimic.procedureevents_mv.CGID|fhir.procedure.performer|\n",
    "|13|mimic.procedureevents_mv.ORDERID| fhir.procedure.basedOn|\n",
    "|14|mimic.procedureevents_mv.LINKORDERID|fhir.procedure.basedOn_linked|\n",
    "|15|mimic.procedureevents_mv.ORDERCATEGORYNAME| fhir.procedure.category_order_name|\n",
    "|16|mimic.procedureevents_mv.SECONDARYORDERCATEGORYNAME|fhir.procedure.category_secOrder_name|\n",
    "|17|mimic.procedureevents_mv.ORDERCATEGORYDESCRIPTION|fhir.procedure.category_order_description|\n",
    "|18|mimic.procedureevents_mv.ISOPENBAG| fhir.procedure.usedReference_openBag|\n",
    "|19|mimic.procedureevents_mv.CONTINUEINEXTDEPT| fhir.procedure.report_contExtDep|\n",
    "|20|mimic.procedureevents_mv.CANCELREASON| fhir.procedure.report_cancelReason|\n",
    "|21|mimic.procedureevents_mv.STATUSDESCRIPTION| fhir.procedure.status|\n",
    "|22|mimic.procedureevents_mv.COMMENTS_EDITEDBY| fhir.procedure.report_editedBy|\n",
    "|23|mimic.procedureevents_mv.COMMENTS_CANCELEDBY| fhir.procedure.report_canceledBy|\n",
    "|24|mimic.procedureevents_mv.COMMENTS_DATE| fhir.procedure.report_canceledDate|\n",
    "|25|mimic.d_items.(LABEL+DBSOURCE+PARAM_TYPE)|mimic.procedure.note|\n",
    "|26|mimic.d_items.CATEGORY|mimic.procedure.category|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_procedures_icd(data_path, output_path):\n",
    "    procedures_icd = pd.read_csv(data_path+'PROCEDURES_ICD'+file_ext, compression=compression)\n",
    "    procedures_icd['followUp'] = procedures_icd.groupby('HADM_ID')['ROW_ID'].shift(-1)\n",
    "    procedures_icd.rename(columns={'ROW_ID':'identifier',\n",
    "                                   'SUBJECT_ID':'subject',\n",
    "                                   'HADM_ID':'encounter',\n",
    "                                   'ICD9_CODE':'code'}, inplace=True)\n",
    "\n",
    "    procedures_icd.to_csv(output_path+'procedure_icd9.csv.gz', compression='gzip', index=False)\n",
    "    return procedures_icd\n",
    "\n",
    "procedure_icd9 = transform_procedures_icd(data_path, output_path)\n",
    "procedure_icd9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_procedurevents_mv(data_path, output_path):\n",
    "    procedurevents_mv = pd.read_csv(data_path+'PROCEDUREEVENTS_MV'+file_ext, compression=compression)\n",
    "    d_items = pd.read_csv(data_path+'D_ITEMS'+file_ext, compression=compression, index_col=0)\n",
    "    \n",
    "    procedurevents_mv = pd.merge(procedurevents_mv, d_items, on='ITEMID')\n",
    "    \n",
    "    procedurevents_mv.STARTTIME = pd.to_datetime(procedurevents_mv.STARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    procedurevents_mv.ENDTIME = pd.to_datetime(procedurevents_mv.ENDTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    procedurevents_mv.COMMENTS_DATE = pd.to_datetime(procedurevents_mv.COMMENTS_DATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "    \n",
    "    procedurevents_mv['note'] = procedurevents_mv['LABEL'] + ' ' + procedurevents_mv['DBSOURCE'] + ' ' + procedurevents_mv['PARAM_TYPE']\n",
    "    \n",
    "    procedurevents_mv.drop(['LABEL', 'PARAM_TYPE', 'STORETIME', 'ABBREVIATION', 'DBSOURCE', 'LINKSTO', 'CONCEPTID', 'UNITNAME'], axis=1, inplace=True)\n",
    "\n",
    "    procedurevents_mv.rename(columns={'ROW_ID':'identifier',\n",
    "                                      'SUBJECT_ID':'subject',\n",
    "                                      'HADM_ID':'encounter',\n",
    "                                      'ICUSTAY_ID':'partOf',\n",
    "                                      'STARTTIME':'performedRange_start',\n",
    "                                      'ENDTIME':'performedRange_end',\n",
    "                                      'ITEMID':'code',\n",
    "                                      'VALUE':'outcome_value',\n",
    "                                      'VALUEUOM':'outcome_unit',\n",
    "                                      'LOCATION':'location_name',\n",
    "                                      'LOCATIONCATEGORY':'location_category',\n",
    "                                      'CGID':'performer',\n",
    "                                      'ORDERID':'basedOn',\n",
    "                                      'LINKORDERID':'basedOn_linked',\n",
    "                                      'ORDERCATEGORYNAME':'category_order_name',\n",
    "                                      'SECONDARYORDERCATEGORYNAME':'category_secOrder_name',\n",
    "                                      'ORDERCATEGORYDESCRIPTION':'category_order_description',\n",
    "                                      'ISOPENBAG':'usedReference_openBag',\n",
    "                                      'CONTINUEINNEXTDEPT':'report_contNextDep',\n",
    "                                      'CANCELREASON':'report_cancelReason',\n",
    "                                      'STATUSDESCRIPTION':'status',\n",
    "                                      'COMMENTS_EDITEDBY':'report_editedBy',\n",
    "                                      'COMMENTS_CANCELEDBY':'report_canceledBy',\n",
    "                                      'COMMENTS_DATE':'report_canceledDate',\n",
    "                                      'CATEGORY':'category'\n",
    "                                     }, inplace=True)\n",
    "\n",
    "    procedurevents_mv.to_csv(output_path+'procedure_mv.csv.gz', compression='gzip', index=False)\n",
    "    return procedurevents_mv\n",
    "\n",
    "procedure_mv = transform_procedurevents_mv(data_path, output_path)\n",
    "procedure_mv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.specimen table\n",
    "\n",
    "#### OUTPUTEVENTS MAPPING:<br>\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.outputevents:ROW_ID | fhir.specimen.identifier|\n",
    "|2|mimic.outputevents.SUBJECT_ID | fhir.specimen.subject|\n",
    "|3|mimic.outputevents.HADM_ID | fhir.specimen.request_encounter_admission|\n",
    "|4|mimic.outputevents.ICUSTAY_ID | fhir.specimen.request_encounter_icustay|\n",
    "|5|mimic.outputevents.ITEMID | fhir.specimen.type_code|\n",
    "|6|mimic.d_items.CATEGORY | fhir.specimen.type_category|\n",
    "|7|mimic.outputevents.CGID | fhir.specimen.collection_collector|\n",
    "|8|mimic.outputevents.CHARTTIME | fhir.specimen.collection_dateTime|\n",
    "|9|mimic.outputevents.VALUE | fhir.specimen.collection_quantity|\n",
    "|10|mimic.outputevents.VALUEUOM | fhir.specimen.collection_unit|\n",
    "|11|mimic.outputevents.NEWBOTTLE | fhir.specimen.collection_newBottle|\n",
    "|12|mimic.outputevents.(STOPPED+ISERROR) | fhir.specimen.status|\n",
    "|13|mimic.d_items.(LABEL+DBSOURCE+PARAM_TYPE) | fhir.specimen.note|\n",
    "\n",
    "#### MICROBIOLOGYEVENTS MAPPING:<br>\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.microbiologyevents.ROW_ID| fhir.specimen.identifier|\n",
    "|2|mimic.microbiologyevents.SUBJECT_ID | fhir.specimen.subject|\n",
    "|3|mimic.microbiologyevents.HADM_ID | fhir.specimen.request_encounter_admission|\n",
    "|4|mimic.microbiologyevents.CHARTTIME | fhir.specimen.collection_dateTime|\n",
    "|5|mimic.microbiologyevents.SPEC_ITEMID | fhir.specimen.type_code|\n",
    "|6|mimic.microbiologyevents.SPEC_TYPE_DESC | fhir.specimen.type_name|\n",
    "|7|mimic.d_items(on SPEC).CATEGORY | fhir.specimen.type_category|\n",
    "|8|mimic.microbiologyevents.ORG_ITEMID | fhir.specimen.method_bact_code|\n",
    "|9|mimic.microbiologyevents.ORG_NAME | fhir.specimen.method_bact_name|\n",
    "|10|mimic.microbiologyevents.ISOLATE_NUM | fhir.specimen.method_colNum|\n",
    "|11|mimic.microbiologyevents.AB_ITEMID | fhir.specimen.method_antibiotic_code|\n",
    "|12|mimic.microbiologyevents.AB_NAME | fhir.specimen.method_antibiotic_name|\n",
    "|13|mimic.microbiologyevents.DILUTION_TEXT | fhir.specimen.method_dilution_description|\n",
    "|14|mimic.microbiologyevents.DILUTION_COMPARISON | fhir.specimen.method_dilution_comp|\n",
    "|15|mimic.microbiologyevents.DILUTION_VALUE | fhir.specimen.method_dilution_value|\n",
    "|16|mimic.microbiologyevents.INTERPRETATION | fhir.specimen.note_interpretation|\n",
    "|17|mimic.d_items_(SPEC/ORG/AB).(LABEL+PARAM_TYPE+DBSOURCE) | fhir.specimen.note|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_outputevents(data_path, output_path):\n",
    "    outputevents = pd.read_csv(data_path+'OUTPUTEVENTS'+file_ext, compression=compression)\n",
    "    d_items = pd.read_csv(data_path+'D_ITEMS'+file_ext, compression=compression, index_col=0)\n",
    "    \n",
    "    specimen_oe = pd.merge(outputevents, d_items, on='ITEMID')\n",
    "    specimen_oe.CHARTTIME = pd.to_datetime(specimen_oe.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "    # Replace NaN in columns with empty strings so that concatenation in notes works\n",
    "    specimen_oe['PARAM_TYPE'].replace(np.NaN, '', regex=True, inplace=True)\n",
    "    specimen_oe['note'] = specimen_oe['LABEL'] + ' ' + specimen_oe['DBSOURCE'] + ' ' + specimen_oe['PARAM_TYPE']\n",
    "\n",
    "    # Combine STOPPED and ISERROR column, Errorneous notes entries will be eliminated later on\n",
    "    specimen_oe.loc[specimen_oe.ISERROR==1,'STOPPED'] = 'Error'\n",
    "\n",
    "    # Drop Columns not needed anymore\n",
    "    specimen_oe.drop(['LABEL', 'PARAM_TYPE', 'STORETIME', 'ISERROR', 'ABBREVIATION', 'DBSOURCE', 'LINKSTO', 'CONCEPTID', 'UNITNAME'], axis=1, inplace=True)\n",
    "\n",
    "    specimen_oe.rename(columns={'ROW_ID':'identifier',\n",
    "                                'SUBJECT_ID':'subject',\n",
    "                                'HADM_ID':'request_encounter_admission',\n",
    "                                'ICUSTAY_ID':'request_encounter_icustay',\n",
    "                                'ITEMID':'type_code',\n",
    "                                'CATEGORY':'type_category',\n",
    "                                'CGID':'collector',\n",
    "                                'CHARTTIME':'collected_dateTime',\n",
    "                                'VALUE':'collection_quantity',\n",
    "                                'VALUEUOM':'collection_unit',\n",
    "                                'NEWBOTTLE':'collection_newBottle',\n",
    "                                'STOPPED':'status'}, inplace=True)\n",
    "    \n",
    "    specimen_oe = specimen_oe.reindex(columns=['identifier',\n",
    "                                                'subject',\n",
    "                                                'request_encounter_admission',\n",
    "                                                'request_encounter_icustay',\n",
    "                                                'type_code',\n",
    "                                                'type_category',\n",
    "                                                'collection_collector',\n",
    "                                                'collection_dateTime',\n",
    "                                                'collection_quantity',\n",
    "                                                'collection_unit',\n",
    "                                                'collection_newBottle',\n",
    "                                                'status',\n",
    "                                                'note'], copy=False)\n",
    "\n",
    "    specimen_oe.to_csv(output_path+'specimen_oe.csv.gz', compression='gzip', index=False)\n",
    "    return specimen_oe\n",
    "\n",
    "specimen_oe = transform_outputevents(data_path, output_path)\n",
    "specimen_oe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_microbiologyevents(data_path, output_path):\n",
    "    microbiologyevents = pd.read_csv(data_path+'MICROBIOLOGYEVENTS'+file_ext, compression=compression)\n",
    "    d_items = pd.read_csv(data_path+'D_ITEMS'+file_ext, compression=compression, index_col=0)\n",
    "    \n",
    "    specimen_mbe = pd.merge(microbiologyevents, d_items[['ITEMID','LABEL','DBSOURCE','PARAM_TYPE','CATEGORY']], left_on='SPEC_ITEMID', right_on='ITEMID')\n",
    "    specimen_mbe = pd.merge(specimen_mbe, d_items[['ITEMID','LABEL','DBSOURCE','PARAM_TYPE']], left_on='ORG_ITEMID', right_on='ITEMID', suffixes=('','_org'))\n",
    "    specimen_mbe = pd.merge(specimen_mbe, d_items[['ITEMID','LABEL','DBSOURCE','PARAM_TYPE']], left_on='AB_ITEMID', right_on='ITEMID', suffixes=('','_ab'))\n",
    "\n",
    "    specimen_mbe.CHARTTIME = pd.to_datetime(specimen_mbe.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "    # Replace NaN in columns with empty strings so that concatenation in notes works\n",
    "    specimen_mbe['PARAM_TYPE'].replace(np.NaN, '', regex=True, inplace=True)\n",
    "    specimen_mbe['PARAM_TYPE_org'].replace(np.NaN, '', regex=True, inplace=True)\n",
    "    specimen_mbe['PARAM_TYPE_ab'].replace(np.NaN, '', regex=True, inplace=True)\n",
    "\n",
    "    specimen_mbe['note'] = specimen_mbe['LABEL'] + ' ' + specimen_mbe['DBSOURCE'] + ' ' + specimen_mbe['PARAM_TYPE']  + ' ' + specimen_mbe['LABEL_org'] + ' ' + specimen_mbe['DBSOURCE_org'] + ' ' + specimen_mbe['PARAM_TYPE_org']  + ' ' + specimen_mbe['LABEL_ab'] + ' ' + specimen_mbe['DBSOURCE_ab'] + ' ' + specimen_mbe['PARAM_TYPE_ab']\n",
    "\n",
    "    # Drop columns combined to note field\n",
    "    specimen_mbe.drop(['CHARTDATE'], axis=1, inplace=True)\n",
    "    specimen_mbe.drop(['ITEMID', 'LABEL', 'PARAM_TYPE','DBSOURCE'], axis=1, inplace=True)\n",
    "    specimen_mbe.drop(['ITEMID_org', 'LABEL_org', 'PARAM_TYPE_org','DBSOURCE_org'], axis=1, inplace=True)\n",
    "    specimen_mbe.drop(['ITEMID_ab', 'LABEL_ab', 'PARAM_TYPE_ab','DBSOURCE_ab'], axis=1, inplace=True)\n",
    "\n",
    "    specimen_mbe.rename(columns={'ROW_ID':'identifier',\n",
    "                                 'SUBJECT_ID':'subject',\n",
    "                                 'HADM_ID':'request_encounter_admission',\n",
    "                                 'CHARTTIME':'collection_dateTime',\n",
    "                                 'SPEC_ITEMID':'type_code',\n",
    "                                 'SPEC_TYPE_DESC':'type_name',\n",
    "                                 'CATEGORY':'type_category',\n",
    "                                 'ORG_ITEMID':'method_bact_code',\n",
    "                                 'ORG_NAME':'method_bact_name',\n",
    "                                 'ISOLATE_NUM':'method_colNum',\n",
    "                                 'AB_ITEMID':'method_antibiotic_code',\n",
    "                                 'AB_NAME':'method_antibiotic_name',\n",
    "                                 'DILUTION_TEXT':'method_dilution_description',\n",
    "                                 'DILUTION_COMPARISON':'method_dilution_comp',\n",
    "                                 'DILUTION_VALUE':'method_dilution_value',\n",
    "                                 'INTERPRETATION':'note_interpretation'}, inplace=True)\n",
    "\n",
    "    specimen_mbe.to_csv(output_path+'specimen_mbe.csv.gz', compression='gzip', index=False)\n",
    "    return specimen_mbe\n",
    "\n",
    "specimen_mbe = transform_microbiologyevents(data_path, output_path)\n",
    "specimen_mbe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fhir.serviceRequest table\n",
    "\n",
    "#### MAPPING:<br>\n",
    "\n",
    "||Original format | FHIR resource format|\n",
    "|------|:-----|:-----|\n",
    "|1|mimic.services.ROW_ID | fhir.serviceRequest.identifier|\n",
    "|2|mimic.services.SUBJECT_ID | fhir.serviceRequest.subject|\n",
    "|3|mimic.services.HADM_ID | fhir.serviceRequest.encounter|\n",
    "|4|mimic.services.TRANSFERTIME | fhir.serviceRequest.occuranceDateTime|\n",
    "|5|mimic.services.PREV_SERVICE | fhir.serviceRequest.replaces|\n",
    "|6|mimic.services.CURR_SERVICE | fhir.serviceRequest.code_name|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_services(data_path, output_path):\n",
    "    services = pd.read_csv(data_path+'SERVICES'+file_ext, compression=compression)\n",
    "    \n",
    "    services.TRANSFERTIME = pd.to_datetime(services.TRANSFERTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "    services.rename(columns={'ROW_ID':'identifier',\n",
    "                             'SUBJECT_ID':'subject',\n",
    "                             'HADM_ID':'encounter',\n",
    "                             'TRANSFERTIME':'occuranceDateTime',\n",
    "                             'PREV_SERVICE':'replaces',\n",
    "                             'CURR_SERVICE':'code_name'}, inplace=True)\n",
    "    \n",
    "    services.to_csv(output_path+'services.csv.gz', compression='gzip', index=False)\n",
    "    return services\n",
    "\n",
    "serviceRequest = transform_services(data_path, output_path)\n",
    "serviceRequest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
