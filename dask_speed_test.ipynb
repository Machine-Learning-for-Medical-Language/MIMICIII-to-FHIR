{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subjective-example",
   "metadata": {},
   "source": [
    "# Dask vs Pandas speed tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-spiritual",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Dask based on Tornado and first release was in 2015 (before asyncio)\n",
    "\n",
    "https://stackoverflow.com/questions/39861685/does-dask-distributed-use-tornado-coroutines-for-workers-tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "historic-swimming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    table {width: 50%;\n",
       "          }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "    table {width: 50%;\n",
    "          }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-constitution",
   "metadata": {},
   "source": [
    "Articles:\n",
    "+ CSV to Parquet with Dask: <https://mungingdata.com/dask/read-csv-to-parquet/>\n",
    "+ https://towardsdatascience.com/beyond-pandas-spark-dask-vaex-and-other-big-data-technologies-battling-head-to-head-a453a1f8cc13\n",
    "+ https://towardsdatascience.com/make-your-data-processing-code-fly-in-5-minutes-c4998e6da094\n",
    "+ https://stackoverflow.com/questions/47191675/pandas-write-dataframe-to-parquet-format-with-append\n",
    "\n",
    "Issues:\n",
    "+ https://stackoverflow.com/questions/60173358/distributed-worker-memory-use-is-high-but-worker-has-no-data-to-store-to-disk\n",
    "+ https://github.com/dask/distributed/issues/4594\n",
    "\n",
    "Results for my task:\n",
    "+ no reindex for dask\n",
    "+ No \"Try using .loc[row_indexer,col_indexer] = value instead\" warning in dask\n",
    "+ no gzip input for CSV in dask\n",
    "+ dask creates *a lot* of files with *huge* cumulative size\n",
    "+ parquet writing a lot faster than csv\n",
    "+ pyarrow is not working with dask (possible bug)\n",
    "+ No append for parquet for both by default (possible with pandas and pyarrow, but there is a [bug](https://issues.apache.org/jira/browse/ARROW-12080))\n",
    "+ in my case, pandas + pyarrow + snappy is the best choice\n",
    "\n",
    "    \n",
    "|      | from->to         | Compression | Time    | Files, n | Files, size |\n",
    "| :--: | :--------------: | :---------: | :-----: | :------: | :---------: |\n",
    "| dask | csv->csv         | none        | 35 min  | 554      | 56 GB       |\n",
    "| dask | csv.gz->?        | NA          | NA      | NA       | NA          |\n",
    "| pd   | csv->csv         | none        | 55 min  | 1        | 56 GB       |\n",
    "| pd   | csv.gz->csv.gz   | gzip        | 55 min  | 1        | ~5 GB       |\n",
    "|      |                  |             |         |          |             |\n",
    "| dask | csv->pyarrow     | ?           | 11 min  | NA       | NA          |\n",
    "| dask | csv->fastparquet | ?           | 14 min  | 554      | 58 GB       |\n",
    "| pd   | csv->pyarrow     | gzip        | ~28 min | 33       | ~3.7 GB     |\n",
    "| pd   | csv->pyarrow     | brotli      | ~33 min | 33       | ~3 GB       |\n",
    "| pd   | csv->pyarrow     | snappy      | ~20 min | 33       | ~5 GB       |\n",
    "| pd   | csv.gz->pyarrow  | snappy      | ~20 min | 33       | ~5 GB       |\n",
    "| pd   | csv->fastparquet | none        | ~25 min | 33       | ~45 GB      |\n",
    "| pd   | csv->fastparquet | snappy      | ~27 min | 33       | ~7 GB       |\n",
    "| pd   | csv->fastparquet | gzip        | ~37 min | 33       | ~3.7 GB     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-friendship",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import dask.dataframe as dd\n",
    "# from dask.diagnostics import ProgressBar  # single machine progressbar\n",
    "# from dask.distributed import progress  # does it work?\n",
    "from dask.distributed import Client  # has dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/mimic-iii-clinical-database-1.4/'\n",
    "output_path = './data/fhir_out/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-gospel",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "\n",
    "\n",
    "def transform_chartevents(data_path, output_path, chunksize=10**7):\n",
    "    \"\"\" ~6GB RAM in peak consumption with default chunksize\n",
    "    \"\"\"\n",
    "    # delete outputfile if exists\n",
    "    output_filename = output_path+'observation_ce'\n",
    "    Path(output_filename).unlink(missing_ok=True)\n",
    "    \n",
    "    d_items = pd.read_csv(data_path+'D_ITEMS.csv.gz', index_col=0,\n",
    "                      # dropped 'ABBREVIATION', 'LINKSTO', 'CONCEPTID', 'UNITNAME'\n",
    "                      usecols=['ROW_ID', 'ITEMID', 'LABEL', 'DBSOURCE', 'CATEGORY', 'PARAM_TYPE'],\n",
    "                      dtype={'ROW_ID': int, 'ITEMID': int, 'LABEL': str, 'DBSOURCE': 'category',\n",
    "                             'CATEGORY': 'category', 'PARAM_TYPE': str})\n",
    "    \n",
    "    # it is the biggest file ~4GB gzipped, 33GB unpacked, 330M strings\n",
    "    # looks like CareVue and Metavision data should be processed separately\n",
    "    chunk_container =  pd.read_csv(data_path+'CHARTEVENTS.csv.gz',\n",
    "                                   # STORETIME\n",
    "                                   usecols=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'ITEMID', 'CHARTTIME',\n",
    "                                            'STORETIME', 'CGID', 'VALUE', 'VALUENUM', 'VALUEUOM', 'WARNING', 'ERROR',\n",
    "                                            'RESULTSTATUS', 'STOPPED'],\n",
    "                                   dtype={'ROW_ID': int, 'SUBJECT_ID': int, 'HADM_ID': int, 'ICUSTAY_ID': float,\n",
    "                                          'ITEMID': int, 'CGID': float, 'VALUE': str, 'VALUENUM': float, \n",
    "                                          'VALUEUOM': str, 'WARNING': float, 'ERROR': float,\n",
    "                                          'RESULTSTATUS': str, 'STOPPED': str},\n",
    "                                   parse_dates=['CHARTTIME'],\n",
    "                                   chunksize=chunksize)  # 2.67GB for 10**7\n",
    "    \n",
    "    for i, chartevents in enumerate(chunk_container):\n",
    "        # Show progress (~330M strings)\n",
    "        print(f'{i + 1}/{330*10**6 / chunksize}', flush=True, end =\" \")\n",
    "        start_time = time.time()\n",
    "\n",
    "        observation_ce = pd.merge(chartevents, d_items, on='ITEMID')\n",
    "\n",
    "        observation_ce['note'] = observation_ce['LABEL'].str.cat(observation_ce['DBSOURCE'], sep=' ', na_rep='NA')\n",
    "        observation_ce['note'] = observation_ce['note'].str.cat(observation_ce['PARAM_TYPE'], sep=' ', na_rep='')\n",
    "\n",
    "        observation_ce.loc[observation_ce['STOPPED'] == \"D/C'd\", 'RESULTSTATUS'] = 'discharged'\n",
    "        observation_ce.loc[observation_ce['ERROR'] == 1, 'RESULTSTATUS'] = 'Error'\n",
    "        \n",
    "        # New columns to adapt to Chartevents observations\n",
    "        observation_ce['category'] = 'chartevents'  # ????\n",
    "\n",
    "        observation_ce.drop(['LABEL', 'PARAM_TYPE', 'ERROR', 'DBSOURCE', 'STOPPED'], axis=1, inplace=True)\n",
    "\n",
    "        observation_ce.rename(columns={'ROW_ID':'identifier',\n",
    "                                       'SUBJECT_ID':'subject',\n",
    "                                       'HADM_ID':'encounter',                               \n",
    "                                       'ICUSTAY_ID':'partOf',\n",
    "                                       'ITEMID':'code',\n",
    "                                       'CGID':'performer',\n",
    "                                       'CHARTTIME':'effectiveDateTime',\n",
    "                                       'VALUE':'value',\n",
    "                                       'VALUENUM':'value_quantity',\n",
    "                                       'VALUEUOM':'unit',\n",
    "                                       'WARNING':'interpretation',\n",
    "                                       'RESULTSTATUS':'status',\n",
    "                                       'CATEGORY':'category_sub'}, inplace=True)\n",
    "\n",
    "        observation_ce = observation_ce.reindex(columns=['identifier',\n",
    "                                                         'subject', \n",
    "                                                         'encounter', \n",
    "                                                         'partOf', \n",
    "                                                         'code',\n",
    "                                                         'effectiveDateTime',\n",
    "                                                         'performer',\n",
    "                                                         'value',\n",
    "                                                         'value_quantity',\n",
    "                                                         'unit', \n",
    "                                                         'interpretation',\n",
    "                                                         'status',\n",
    "                                                         'note',\n",
    "                                                         'category_sub',\n",
    "                                                         'category'], copy=False)\n",
    "\n",
    "#         observation_ce.to_csv(output_filename + '.csv.gz',\n",
    "#                               compression={'method': 'gzip', 'compresslevel': 1},\n",
    "#                               index=False, mode='a')\n",
    "        \n",
    "#         observation_ce.to_csv(output_filename + '.csv', index=False, mode='a')\n",
    "        \n",
    "        # will create a lot of files, no append mode\n",
    "        observation_ce.to_parquet(f\"{output_filename}_{i}.parquet\", \n",
    "                                  compression='snappy', index=False)\n",
    "\n",
    "        # force free mem, for some reasons without it, RAM ends pretty quick\n",
    "        gc.collect()\n",
    "        # show execution time per chunk\n",
    "        print(f\"--- {time.time() - start_time} seconds ---\", flush=True)\n",
    "\n",
    "        \n",
    "transform_chartevents(data_path, output_path)\n",
    "# observation_ce = transform_chartevents(data_path, output_path)\n",
    "# observation_ce.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-removal",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_items = pd.read_csv(data_path+'D_ITEMS.csv.gz', index_col=0,\n",
    "                  # dropped 'ABBREVIATION', 'LINKSTO', 'CONCEPTID', 'UNITNAME'\n",
    "                  usecols=['ROW_ID', 'ITEMID', 'LABEL', 'DBSOURCE', 'CATEGORY', 'PARAM_TYPE'],\n",
    "                  dtype={'ROW_ID': int, 'ITEMID': int, 'LABEL': str, 'DBSOURCE': 'category',\n",
    "                         'CATEGORY': 'category', 'PARAM_TYPE': str})\n",
    "\n",
    "d_items.set_index('ITEMID', inplace=True)\n",
    "d_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client(threads_per_worker=2, n_workers=4, memory_limit='2.5GB')\n",
    "client = Client()  # it will consume all memory by default\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning gzip compression does not support breaking apart files\n",
    "# Q: what default blocksize? A: https://github.com/dask/dask/pull/1328 (32M)\n",
    "# max 64e6 bytes ~61MB\n",
    "df =  dd.read_csv(data_path + 'CHARTEVENTS' + '.csv',\n",
    "                  usecols=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'ITEMID', 'CHARTTIME',\n",
    "                           'STORETIME', 'CGID', 'VALUE', 'VALUENUM', 'VALUEUOM', 'WARNING', 'ERROR',\n",
    "                           'RESULTSTATUS', 'STOPPED'],\n",
    "                  dtype={'ROW_ID': int, 'SUBJECT_ID': int, 'HADM_ID': int, 'ICUSTAY_ID': float,\n",
    "                         'ITEMID': int, 'CGID': float, 'VALUE': str, 'VALUENUM': float, \n",
    "                         'VALUEUOM': str, 'WARNING': float, 'ERROR': float,\n",
    "                         'RESULTSTATUS': str, 'STOPPED': str},\n",
    "                  parse_dates=['CHARTTIME'])\n",
    "#                   blocksize=64e6) \n",
    "\n",
    "# it will take a while, like 4min\n",
    "# raises \"TypeError: memoryview: cannot cast view with zeros in shape or strides\"\n",
    "# at the final stages\n",
    "# df = df.set_index('ITEMID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_ce = dd.merge(df, d_items, left_on='ITEMID', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = output_path+'observation_ce'\n",
    "\n",
    "observation_ce['note'] = observation_ce['LABEL'].str.cat(observation_ce['DBSOURCE'], sep=' ', na_rep='NA')\n",
    "observation_ce['note'] = observation_ce['note'].str.cat(observation_ce['PARAM_TYPE'], sep=' ', na_rep='')\n",
    "\n",
    "# No \"Try using .loc[row_indexer,col_indexer] = value instead\" warning\n",
    "# .loc way is not implemented/usable in dask\n",
    "mask = observation_ce['STOPPED'] == \"D/C'd\"\n",
    "observation_ce[mask]['RESULTSTATUS'] = 'discharged'\n",
    "mask2 = observation_ce['ERROR'] == 1\n",
    "observation_ce[mask]['RESULTSTATUS'] = 'Error'\n",
    "\n",
    "# New columns to adapt to Chartevents observations\n",
    "observation_ce['category'] = 'chartevents'  # ????\n",
    "\n",
    "observation_ce.drop(columns=['LABEL', 'PARAM_TYPE', 'ERROR', 'DBSOURCE', 'STOPPED'])\n",
    "\n",
    "observation_ce.rename(columns={'ROW_ID':'identifier',\n",
    "                               'SUBJECT_ID':'subject',\n",
    "                               'HADM_ID':'encounter',                               \n",
    "                               'ICUSTAY_ID':'partOf',\n",
    "                               'ITEMID':'code',\n",
    "                               'CGID':'performer',\n",
    "                               'CHARTTIME':'effectiveDateTime',\n",
    "                               'VALUE':'value',\n",
    "                               'VALUENUM':'value_quantity',\n",
    "                               'VALUEUOM':'unit',\n",
    "                               'WARNING':'interpretation',\n",
    "                               'RESULTSTATUS':'status',\n",
    "                               'CATEGORY':'category_sub'})\n",
    "\n",
    "# dask loves parquet\n",
    "# using pyarrow: https://github.com/dask/dask/issues/6587\n",
    "# observation_ce.to_csv(output_filename + '.csv')\n",
    "observation_ce.to_parquet(output_filename + '.parquet', schema=\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-marking",
   "metadata": {},
   "source": [
    "Attemt to use `map_partition(foo)` failed with issues from header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/41806850/dask-difference-between-client-persist-and-client-compute\n",
    "# observation_ce.persist()  # in bg\n",
    "# observation_ce.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-nylon",
   "metadata": {},
   "source": [
    "## parquet to single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-young",
   "metadata": {},
   "source": [
    "+ https://arrow.apache.org/docs/python/dataset.html\n",
    "+ https://stackoverflow.com/questions/47191675/pandas-write-dataframe-to-parquet-format-with-append\n",
    "+ **https://stackoverflow.com/questions/47113813/using-pyarrow-how-do-you-append-to-parquet-file**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-dispatch",
   "metadata": {},
   "source": [
    "### Use dataset (dir with a bunch of small parquet files)\n",
    "\n",
    "it need to be dowloaded into the memory in order to save as a single file\n",
    "\n",
    "https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Dataset.html#pyarrow.dataset.Dataset.to_table\n",
    ">Note that this method reads all the selected data from the dataset into memory.\n",
    "\n",
    "It has problems: https://issues.apache.org/jira/browse/ARROW-12080\n",
    ">The first table schema becomes a common schema for the full Dataset. It is a problem if all values in the first table is na.\n",
    "> It ignores pandas dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-height",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd \n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = './data/mimic-iii-clinical-database-1.4/'\n",
    "output = \"mydataset.parquet\"\n",
    "if Path(output).exists():\n",
    "    shutil.rmtree(output)\n",
    "\n",
    "d_items = pd.read_csv(data_path+'D_ITEMS.csv.gz', index_col=0,\n",
    "                  # dropped 'ABBREVIATION', 'LINKSTO', 'CONCEPTID', 'UNITNAME'\n",
    "                  usecols=['ROW_ID', 'ITEMID', 'LABEL', 'DBSOURCE', 'CATEGORY', 'PARAM_TYPE'],\n",
    "                  dtype={'ROW_ID': int, 'ITEMID': int, 'LABEL': str, 'DBSOURCE': str,\n",
    "                         'CATEGORY': str, 'PARAM_TYPE': str}, chunksize=1000)\n",
    "\n",
    "for i, chunk in enumerate(d_items):\n",
    "    # create a parquet table from your dataframe\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "    if i == 0:\n",
    "        schema = pa.Schema.from_pandas(chunk)\n",
    "#         schema = table.schema\n",
    "    # write direct to your parquet file\n",
    "    pq.write_to_dataset(table, root_path=output)\n",
    "    \n",
    "    \n",
    "print(dataset.schema.types)\n",
    "# try to convert dataset to a single file\n",
    "# dataset = ds.dataset(output, schema=schema)\n",
    "dataset = ds.dataset(output)\n",
    "# it will load all into the memory\n",
    "dataset.to_table()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-national",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-latvia",
   "metadata": {},
   "source": [
    "#### Test case\n",
    "will crash at random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-resolution",
   "metadata": {},
   "source": [
    "```python\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "output='./tmp.parquet'\n",
    "if Path(output).exists():\n",
    "    shutil.rmtree(output)\n",
    "\n",
    "df = pd.DataFrame({'A': [np.nan, np.nan, '3', np.nan],\n",
    "                   'B': ['1', '2', '3', np.nan]},\n",
    "                  dtype=str).to_csv('tmp.csv', index=False)\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv('tmp.csv', dtype={'A': str, 'B': str}, chunksize=1)):\n",
    "    # create a parquet table from your dataframe\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "    print(table.schema.types)\n",
    "    # write direct to your parquet file\n",
    "    pq.write_to_dataset(table, root_path=output)\n",
    "\n",
    "# it will crash randomly\n",
    "dataset = ds.dataset('./tmp.parquet')\n",
    "print('dataset schema: ', dataset.schema.types)\n",
    "print(dataset.to_table().to_pandas())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-atlanta",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-enough",
   "metadata": {},
   "source": [
    "### Use single parquet file from start\n",
    "\n",
    "Has the same bug/problems: https://issues.apache.org/jira/browse/ARROW-12080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-maria",
   "metadata": {},
   "source": [
    "```python\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# will crash with chunksize=1000\n",
    "chunksize=10000 # this is the number of lines\n",
    "data_path = './data/mimic-iii-clinical-database-1.4/'\n",
    "output = 'sample.parquet'\n",
    "Path(output).unlink(missing_ok=True)\n",
    "\n",
    "d_items = pd.read_csv(data_path+'D_ITEMS.csv.gz', index_col=0,\n",
    "                  # dropped 'ABBREVIATION', 'LINKSTO', 'CONCEPTID', 'UNITNAME'\n",
    "                  usecols=['ROW_ID', 'ITEMID', 'LABEL', 'DBSOURCE', 'CATEGORY', 'PARAM_TYPE'],\n",
    "                  dtype={'ROW_ID': int, 'ITEMID': int, 'LABEL': str, 'DBSOURCE': str,\n",
    "                         'CATEGORY': str, 'PARAM_TYPE': str}, chunksize=chunksize)\n",
    "\n",
    "\n",
    "table = pa.Table.from_pandas(next(d_items))\n",
    "with pq.ParquetWriter(output, table.schema) as pqwriter:\n",
    "    pqwriter.write_table(table)\n",
    "    for chunk in d_items:\n",
    "        table = pa.Table.from_pandas(chunk)\n",
    "        pqwriter.write_table(table)\n",
    "\n",
    "    \n",
    "df1 = pd.read_parquet(output)\n",
    "\n",
    "df2 = pd.read_csv(data_path+'D_ITEMS.csv.gz', index_col=0,\n",
    "                  # dropped 'ABBREVIATION', 'LINKSTO', 'CONCEPTID', 'UNITNAME'\n",
    "                  usecols=['ROW_ID', 'ITEMID', 'LABEL', 'DBSOURCE', 'CATEGORY', 'PARAM_TYPE'],\n",
    "                  dtype={'ROW_ID': int, 'ITEMID': int, 'LABEL': str, 'DBSOURCE': str,\n",
    "                         'CATEGORY': str, 'PARAM_TYPE': str})\n",
    "\n",
    "df1.equals(df2)\n",
    "df1.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-prompt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
